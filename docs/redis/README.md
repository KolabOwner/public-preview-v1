# Redis Management & Admin Interface Documentation
## Comprehensive Redis Architecture for Resume Processing Platform

---

## üìã Table of Contents

1. [Redis Architecture Overview](#redis-architecture-overview)
2. [Data Structures & Patterns](#data-structures--patterns)
3. [Key Naming Conventions](#key-naming-conventions)
4. [CRUD Operations](#crud-operations)
5. [Caching Strategies](#caching-strategies)
6. [Queue Management](#queue-management)
7. [Performance Monitoring](#performance-monitoring)
8. [Admin Interface Requirements](#admin-interface-requirements)
9. [Troubleshooting Guide](#troubleshooting-guide)
10. [Best Practices & Optimization](#best-practices--optimization)

---

## üèóÔ∏è Redis Architecture Overview

### Redis Usage in Resume Processing System

The platform utilizes Redis for multiple critical functions:

```typescript
interface RedisArchitecture {
  // Core Components
  caching: {
    parsed_resumes: 'L1 cache for processed documents',
    keyword_analysis: 'Analysis results cache',
    ai_responses: 'AI model response cache',
    user_sessions: 'Session state management'
  },
  
  queuing: {
    processing_queue: 'Resume processing job queue',
    analysis_queue: 'Keyword analysis tasks',
    export_queue: 'PDF generation tasks'
  },
  
  realtime: {
    pubsub_channels: 'WebSocket event distribution',
    progress_tracking: 'Processing progress updates',
    notifications: 'User notification delivery'
  },
  
  persistence: {
    temp_storage: 'Temporary file chunks',
    rate_limiting: 'API rate limit counters',
    metrics: 'Performance metrics storage'
  }
}
```

### Redis Deployment Configuration

```yaml
# Redis Configuration for Resume Processing
redis:
  mode: cluster
  nodes:
    - host: redis-node-1
      port: 6379
      role: master
    - host: redis-node-2
      port: 6379
      role: slave
    - host: redis-node-3
      port: 6379
      role: slave
  
  persistence:
    aof: yes
    aof-use-rdb-preamble: yes
    save:
      - "900 1"      # Save after 900 sec if at least 1 key changed
      - "300 10"     # Save after 300 sec if at least 10 keys changed
      - "60 10000"   # Save after 60 sec if at least 10000 keys changed
  
  memory:
    maxmemory: 4gb
    maxmemory-policy: allkeys-lru
    
  performance:
    tcp-backlog: 511
    timeout: 300
    tcp-keepalive: 60
```

---

## üìä Data Structures & Patterns

### 1. Resume Cache Structure

```typescript
// Key Pattern: resume:cache:{userId}:{resumeId}:{version}
interface ResumeCacheEntry {
  key: string;
  type: 'hash';
  fields: {
    content: string;          // Original file content (base64)
    parsed: string;           // JSON stringified parsed data
    metadata: string;         // RMS metadata JSON
    processingPath: string;   // 'fast' | 'metadata'
    extractedText: string;    // Plain text version
    lastAccessed: string;     // ISO timestamp
    hitCount: string;         // Access counter
  };
  ttl: 3600; // 1 hour
}

// Example Redis commands:
// HSET resume:cache:user123:resume456:v2 content "base64data" parsed "{...}" metadata "{...}"
// EXPIRE resume:cache:user123:resume456:v2 3600
// HINCRBY resume:cache:user123:resume456:v2 hitCount 1
```

### 2. Keyword Analysis Cache

```typescript
// Key Pattern: keywords:{jobId}:{resumeHash}
interface KeywordCacheEntry {
  key: string;
  type: 'hash';
  fields: {
    missingKeywords: string;    // JSON array of missing keywords
    matchingKeywords: string;   // JSON array of matching keywords
    atsScore: string;          // Numeric score (0-100)
    suggestions: string;       // JSON array of suggestions
    analysisVersion: string;   // Algorithm version
    timestamp: string;         // ISO timestamp
    processingTime: string;    // Duration in ms
  };
  ttl: 86400; // 24 hours
}

// Sorted set for keyword frequency tracking
// Key Pattern: keywords:frequency:{domain}
interface KeywordFrequency {
  key: string;
  type: 'sorted-set';
  members: {
    keyword: string;
    score: number; // Frequency count
  }[];
}
```

### 3. Processing Queue Structure

```typescript
// List-based queue with backup
// Key Pattern: queue:processing:{priority}
interface ProcessingQueue {
  key: string;
  type: 'list';
  structure: {
    job: {
      id: string;
      userId: string;
      fileUrl: string;
      priority: number;
      retries: number;
      createdAt: string;
      metadata: object;
    };
  };
}

// Processing status tracking
// Key Pattern: processing:status:{jobId}
interface ProcessingStatus {
  key: string;
  type: 'hash';
  fields: {
    status: string;        // 'pending' | 'processing' | 'completed' | 'failed'
    progress: string;      // 0-100
    stage: string;         // Current processing stage
    startTime: string;     // ISO timestamp
    lastUpdate: string;    // ISO timestamp
    workerId: string;      // Processing worker ID
    error?: string;        // Error message if failed
  };
  ttl: 7200; // 2 hours
}
```

### 4. Rate Limiting Structure

```typescript
// Key Pattern: ratelimit:{service}:{userId}:{window}
interface RateLimitEntry {
  key: string;
  type: 'string';
  value: number; // Request count
  ttl: number;   // Window duration
}

// Sliding window implementation
// Key Pattern: ratelimit:sliding:{service}:{userId}
interface SlidingWindowRateLimit {
  key: string;
  type: 'sorted-set';
  members: {
    timestamp: string;
    score: number; // Unix timestamp
  }[];
}
```

### 5. Session Management

```typescript
// Key Pattern: session:{sessionId}
interface SessionEntry {
  key: string;
  type: 'hash';
  fields: {
    userId: string;
    resumeId: string;
    currentJob: string;
    preferences: string;    // JSON user preferences
    lastActivity: string;   // ISO timestamp
  };
  ttl: 1800; // 30 minutes, refreshed on activity
}
```

### 6. Metrics and Analytics

```typescript
// Time-series data for metrics
// Key Pattern: metrics:{metric}:{timestamp}
interface MetricsEntry {
  key: string;
  type: 'hash';
  fields: {
    count: string;
    sum: string;
    min: string;
    max: string;
    avg: string;
    p50: string;
    p95: string;
    p99: string;
  };
  ttl: 604800; // 7 days
}

// Real-time metrics using HyperLogLog
// Key Pattern: metrics:unique:{metric}:{date}
interface UniqueMetrics {
  key: string;
  type: 'hyperloglog';
  purpose: 'Count unique users/operations';
}
```

---

## üîë Key Naming Conventions

### Hierarchical Key Structure

```typescript
const KEY_PATTERNS = {
  // Cache Keys
  cache: {
    resume: 'resume:cache:{userId}:{resumeId}:{version}',
    keywords: 'keywords:{jobId}:{resumeHash}',
    ai_response: 'ai:cache:{model}:{promptHash}',
    user_data: 'user:cache:{userId}:{dataType}'
  },
  
  // Queue Keys
  queues: {
    processing: 'queue:processing:{priority}',
    analysis: 'queue:analysis:{priority}',
    export: 'queue:export:{type}',
    dlq: 'queue:dlq:{originalQueue}' // Dead letter queue
  },
  
  // Status Keys
  status: {
    processing: 'processing:status:{jobId}',
    analysis: 'analysis:status:{analysisId}',
    worker: 'worker:status:{workerId}'
  },
  
  // Lock Keys
  locks: {
    processing: 'lock:processing:{resourceId}',
    exclusive: 'lock:exclusive:{operation}:{resourceId}'
  },
  
  // Metrics Keys
  metrics: {
    counter: 'metrics:counter:{metric}:{period}',
    gauge: 'metrics:gauge:{metric}',
    histogram: 'metrics:histogram:{metric}:{bucket}',
    unique: 'metrics:unique:{metric}:{date}'
  },
  
  // Configuration Keys
  config: {
    feature_flags: 'config:features:{feature}',
    settings: 'config:settings:{component}',
    limits: 'config:limits:{resource}'
  }
};

// Key generation helpers
export class RedisKeyGenerator {
  static generateCacheKey(type: string, ...params: string[]): string {
    const pattern = KEY_PATTERNS.cache[type];
    if (!pattern) throw new Error(`Unknown cache type: ${type}`);
    
    return this.interpolatePattern(pattern, params);
  }
  
  static generateQueueKey(queue: string, priority: number = 0): string {
    return `queue:${queue}:${priority}`;
  }
  
  static generateLockKey(resource: string, operation: string = 'default'): string {
    return `lock:${operation}:${resource}:${Date.now()}`;
  }
  
  private static interpolatePattern(pattern: string, params: string[]): string {
    let result = pattern;
    let paramIndex = 0;
    
    result = result.replace(/{(\w+)}/g, (match, key) => {
      return params[paramIndex++] || match;
    });
    
    return result;
  }
}
```

---

## üõ†Ô∏è CRUD Operations

### Resume Cache Operations

```typescript
export class ResumeCacheOperations {
  constructor(private redis: Redis) {}

  // CREATE/UPDATE
  async saveResumeCache(
    userId: string,
    resumeId: string,
    data: ResumeCacheData
  ): Promise<void> {
    const key = RedisKeyGenerator.generateCacheKey('resume', userId, resumeId, data.version);
    
    const pipeline = this.redis.pipeline();
    
    // Set all hash fields
    pipeline.hset(key, {
      content: data.content,
      parsed: JSON.stringify(data.parsed),
      metadata: JSON.stringify(data.metadata),
      processingPath: data.processingPath,
      extractedText: data.extractedText,
      lastAccessed: new Date().toISOString(),
      hitCount: '0'
    });
    
    // Set expiration
    pipeline.expire(key, 3600); // 1 hour
    
    // Add to index for user
    pipeline.sadd(`resume:index:${userId}`, resumeId);
    
    await pipeline.exec();
  }

  // READ
  async getResumeCache(
    userId: string,
    resumeId: string,
    version: string = 'latest'
  ): Promise<ResumeCacheData | null> {
    const key = RedisKeyGenerator.generateCacheKey('resume', userId, resumeId, version);
    
    // Get data and update access stats atomically
    const pipeline = this.redis.pipeline();
    pipeline.hgetall(key);
    pipeline.hincrby(key, 'hitCount', 1);
    pipeline.hset(key, 'lastAccessed', new Date().toISOString());
    pipeline.expire(key, 3600); // Refresh TTL
    
    const results = await pipeline.exec();
    const data = results[0][1] as any;
    
    if (!data || Object.keys(data).length === 0) {
      return null;
    }
    
    return {
      content: data.content,
      parsed: JSON.parse(data.parsed),
      metadata: JSON.parse(data.metadata),
      processingPath: data.processingPath,
      extractedText: data.extractedText,
      hitCount: parseInt(data.hitCount),
      lastAccessed: data.lastAccessed
    };
  }

  // DELETE
  async deleteResumeCache(userId: string, resumeId: string): Promise<void> {
    // Get all versions
    const pattern = `resume:cache:${userId}:${resumeId}:*`;
    const keys = await this.redis.keys(pattern);
    
    if (keys.length > 0) {
      const pipeline = this.redis.pipeline();
      keys.forEach(key => pipeline.del(key));
      pipeline.srem(`resume:index:${userId}`, resumeId);
      await pipeline.exec();
    }
  }

  // BULK OPERATIONS
  async bulkGetResumes(userId: string): Promise<ResumeSummary[]> {
    const resumeIds = await this.redis.smembers(`resume:index:${userId}`);
    
    if (resumeIds.length === 0) return [];
    
    const pipeline = this.redis.pipeline();
    resumeIds.forEach(resumeId => {
      const key = RedisKeyGenerator.generateCacheKey('resume', userId, resumeId, 'latest');
      pipeline.hget(key, 'metadata');
    });
    
    const results = await pipeline.exec();
    
    return results
      .map((result, index) => {
        if (result[1]) {
          const metadata = JSON.parse(result[1] as string);
          return {
            resumeId: resumeIds[index],
            ...metadata
          };
        }
        return null;
      })
      .filter(Boolean) as ResumeSummary[];
  }
}
```

### Keyword Analysis Operations

```typescript
export class KeywordCacheOperations {
  constructor(private redis: Redis) {}

  async saveAnalysis(
    jobId: string,
    resumeHash: string,
    analysis: KeywordAnalysisResult
  ): Promise<void> {
    const key = `keywords:${jobId}:${resumeHash}`;
    
    await this.redis.hset(key, {
      missingKeywords: JSON.stringify(analysis.missingKeywords),
      matchingKeywords: JSON.stringify(analysis.matchingKeywords),
      atsScore: analysis.atsScore.toString(),
      suggestions: JSON.stringify(analysis.suggestions),
      analysisVersion: analysis.version,
      timestamp: new Date().toISOString(),
      processingTime: analysis.processingTime.toString()
    });
    
    await this.redis.expire(key, 86400); // 24 hours
    
    // Update keyword frequency
    await this.updateKeywordFrequency(analysis);
  }

  async getAnalysis(
    jobId: string,
    resumeHash: string
  ): Promise<KeywordAnalysisResult | null> {
    const key = `keywords:${jobId}:${resumeHash}`;
    const data = await this.redis.hgetall(key);
    
    if (!data || Object.keys(data).length === 0) {
      return null;
    }
    
    return {
      missingKeywords: JSON.parse(data.missingKeywords),
      matchingKeywords: JSON.parse(data.matchingKeywords),
      atsScore: parseFloat(data.atsScore),
      suggestions: JSON.parse(data.suggestions),
      version: data.analysisVersion,
      timestamp: data.timestamp,
      processingTime: parseInt(data.processingTime)
    };
  }

  private async updateKeywordFrequency(analysis: KeywordAnalysisResult): Promise<void> {
    const domain = this.extractDomain(analysis);
    const key = `keywords:frequency:${domain}`;
    
    const pipeline = this.redis.pipeline();
    
    // Increment frequency for all keywords
    [...analysis.missingKeywords, ...analysis.matchingKeywords].forEach(keyword => {
      pipeline.zincrby(key, 1, keyword.text);
    });
    
    await pipeline.exec();
  }

  async getTopKeywords(domain: string, limit: number = 50): Promise<KeywordFrequency[]> {
    const key = `keywords:frequency:${domain}`;
    const results = await this.redis.zrevrange(key, 0, limit - 1, 'WITHSCORES');
    
    const keywords: KeywordFrequency[] = [];
    for (let i = 0; i < results.length; i += 2) {
      keywords.push({
        keyword: results[i],
        frequency: parseInt(results[i + 1])
      });
    }
    
    return keywords;
  }
}
```

### Queue Operations

```typescript
export class QueueOperations {
  constructor(private redis: Redis) {}

  // ENQUEUE
  async enqueue(
    queueName: string,
    job: ProcessingJob,
    priority: number = 0
  ): Promise<string> {
    const queueKey = `queue:${queueName}:${priority}`;
    const jobData = JSON.stringify(job);
    
    // Add to queue
    await this.redis.lpush(queueKey, jobData);
    
    // Set job status
    await this.setJobStatus(job.id, 'pending', { queueName, priority });
    
    // Publish notification for workers
    await this.redis.publish(`queue:notification:${queueName}`, job.id);
    
    return job.id;
  }

  // DEQUEUE
  async dequeue(
    queueName: string,
    priority: number = 0,
    timeout: number = 0
  ): Promise<ProcessingJob | null> {
    const queueKey = `queue:${queueName}:${priority}`;
    const backupKey = `queue:processing:${queueName}`;
    
    // Atomic move from queue to processing
    const result = await this.redis.brpoplpush(queueKey, backupKey, timeout);
    
    if (!result) return null;
    
    const job = JSON.parse(result) as ProcessingJob;
    
    // Update job status
    await this.setJobStatus(job.id, 'processing', {
      workerId: process.env.WORKER_ID,
      startTime: new Date().toISOString()
    });
    
    return job;
  }

  // ACKNOWLEDGE (Remove from backup queue)
  async acknowledge(queueName: string, job: ProcessingJob): Promise<void> {
    const backupKey = `queue:processing:${queueName}`;
    
    await this.redis.lrem(backupKey, 1, JSON.stringify(job));
    await this.setJobStatus(job.id, 'completed', {
      completedTime: new Date().toISOString()
    });
  }

  // REJECT (Move to DLQ)
  async reject(
    queueName: string,
    job: ProcessingJob,
    error: string
  ): Promise<void> {
    const backupKey = `queue:processing:${queueName}`;
    const dlqKey = `queue:dlq:${queueName}`;
    
    // Remove from backup
    await this.redis.lrem(backupKey, 1, JSON.stringify(job));
    
    // Add to DLQ with error info
    const dlqJob = {
      ...job,
      error,
      failedAt: new Date().toISOString(),
      originalQueue: queueName
    };
    
    await this.redis.lpush(dlqKey, JSON.stringify(dlqJob));
    await this.setJobStatus(job.id, 'failed', { error });
  }

  // MONITORING
  async getQueueStats(queueName: string): Promise<QueueStats> {
    const priorities = [0, 1, 2]; // high, medium, low
    const stats: QueueStats = {
      pending: 0,
      processing: 0,
      failed: 0,
      completed: 0,
      byPriority: {}
    };
    
    // Get pending counts by priority
    for (const priority of priorities) {
      const count = await this.redis.llen(`queue:${queueName}:${priority}`);
      stats.pending += count;
      stats.byPriority[priority] = count;
    }
    
    // Get processing count
    stats.processing = await this.redis.llen(`queue:processing:${queueName}`);
    
    // Get failed count
    stats.failed = await this.redis.llen(`queue:dlq:${queueName}`);
    
    return stats;
  }

  private async setJobStatus(
    jobId: string,
    status: string,
    additionalData: Record<string, any> = {}
  ): Promise<void> {
    const key = `processing:status:${jobId}`;
    
    await this.redis.hset(key, {
      status,
      lastUpdate: new Date().toISOString(),
      ...additionalData
    });
    
    await this.redis.expire(key, 7200); // 2 hours
  }
}
```

---

## üöÄ Caching Strategies

### Multi-Layer Cache Implementation

```typescript
export class MultiLayerCache {
  private readonly layers = {
    L1: { ttl: 300, maxSize: 1000 },    // 5 minutes, hot data
    L2: { ttl: 3600, maxSize: 10000 },  // 1 hour, warm data
    L3: { ttl: 86400, maxSize: 100000 } // 24 hours, cold data
  };

  async get<T>(key: string): Promise<T | null> {
    // Check L1 (hot cache)
    const l1Key = `cache:l1:${key}`;
    let value = await this.redis.get(l1Key);
    
    if (value) {
      this.metrics.increment('cache.l1.hit');
      return JSON.parse(value);
    }
    
    // Check L2 (warm cache)
    const l2Key = `cache:l2:${key}`;
    value = await this.redis.get(l2Key);
    
    if (value) {
      this.metrics.increment('cache.l2.hit');
      // Promote to L1
      await this.promote(key, value, 'L1');
      return JSON.parse(value);
    }
    
    // Check L3 (cold cache)
    const l3Key = `cache:l3:${key}`;
    value = await this.redis.get(l3Key);
    
    if (value) {
      this.metrics.increment('cache.l3.hit');
      // Promote to L2 and L1
      await this.promote(key, value, 'L2');
      await this.promote(key, value, 'L1');
      return JSON.parse(value);
    }
    
    this.metrics.increment('cache.miss');
    return null;
  }

  async set<T>(key: string, value: T, options: CacheOptions = {}): Promise<void> {
    const serialized = JSON.stringify(value);
    const size = Buffer.byteLength(serialized);
    
    // Determine which layer based on size and importance
    const layer = this.selectLayer(size, options.importance);
    
    const cacheKey = `cache:${layer.toLowerCase()}:${key}`;
    await this.redis.setex(cacheKey, this.layers[layer].ttl, serialized);
    
    // Update size tracking
    await this.updateSizeTracking(layer, size);
    
    // Evict if necessary
    await this.evictIfNeeded(layer);
  }

  private async promote(key: string, value: string, toLayer: string): Promise<void> {
    const cacheKey = `cache:${toLayer.toLowerCase()}:${key}`;
    await this.redis.setex(cacheKey, this.layers[toLayer].ttl, value);
  }

  private async evictIfNeeded(layer: string): Promise<void> {
    const sizeKey = `cache:size:${layer.toLowerCase()}`;
    const currentSize = await this.redis.get(sizeKey);
    
    if (parseInt(currentSize || '0') > this.layers[layer].maxSize * 1024 * 1024) {
      // Implement LRU eviction
      await this.evictLRU(layer);
    }
  }
}
```

### Cache Warming Strategy

```typescript
export class CacheWarmer {
  async warmCache(): Promise<void> {
    // Warm frequently accessed resumes
    await this.warmResumeCache();
    
    // Warm popular keyword analyses
    await this.warmKeywordCache();
    
    // Warm user sessions
    await this.warmSessionCache();
  }

  private async warmResumeCache(): Promise<void> {
    // Get most accessed resumes from metrics
    const popularResumes = await this.redis.zrevrange(
      'metrics:resume:access',
      0,
      99,
      'WITHSCORES'
    );
    
    for (let i = 0; i < popularResumes.length; i += 2) {
      const resumeId = popularResumes[i];
      const accessCount = parseInt(popularResumes[i + 1]);
      
      if (accessCount > 10) {
        // Preload into cache
        await this.preloadResume(resumeId);
      }
    }
  }

  private async warmKeywordCache(): Promise<void> {
    // Get recent job analyses
    const recentJobs = await this.redis.lrange('recent:jobs', 0, 49);
    
    for (const jobId of recentJobs) {
      // Preload keyword analysis for common resume combinations
      await this.preloadKeywordAnalysis(jobId);
    }
  }
}
```

---

## üìä Queue Management

### Priority Queue Implementation

```typescript
export class PriorityQueueManager {
  private readonly priorities = {
    HIGH: 0,
    MEDIUM: 1,
    LOW: 2
  };

  async processQueues(queueName: string): Promise<void> {
    while (true) {
      // Process high priority first
      let job = await this.dequeueWithPriority(queueName, this.priorities.HIGH);
      
      if (!job) {
        // Try medium priority
        job = await this.dequeueWithPriority(queueName, this.priorities.MEDIUM);
      }
      
      if (!job) {
        // Try low priority
        job = await this.dequeueWithPriority(queueName, this.priorities.LOW);
      }
      
      if (!job) {
        // No jobs available, wait
        await this.waitForNotification(queueName);
        continue;
      }
      
      // Process job
      await this.processJob(job);
    }
  }

  private async dequeueWithPriority(
    queueName: string,
    priority: number
  ): Promise<ProcessingJob | null> {
    const queueKey = `queue:${queueName}:${priority}`;
    const processingKey = `queue:processing:${queueName}`;
    
    // Atomic move to processing queue
    const jobData = await this.redis.rpoplpush(queueKey, processingKey);
    
    if (!jobData) return null;
    
    return JSON.parse(jobData);
  }

  async monitorQueues(): Promise<QueueHealth> {
    const health: QueueHealth = {
      queues: {},
      workers: {},
      alerts: []
    };
    
    // Check each queue
    for (const queueName of ['processing', 'analysis', 'export']) {
      const stats = await this.getQueueStats(queueName);
      health.queues[queueName] = stats;
      
      // Check for issues
      if (stats.failed > 10) {
        health.alerts.push({
          severity: 'high',
          message: `High failure rate in ${queueName} queue`,
          queue: queueName,
          failedCount: stats.failed
        });
      }
      
      if (stats.pending > 1000) {
        health.alerts.push({
          severity: 'medium',
          message: `Queue backlog in ${queueName}`,
          queue: queueName,
          pendingCount: stats.pending
        });
      }
    }
    
    // Check worker health
    const workers = await this.redis.keys('worker:status:*');
    for (const workerKey of workers) {
      const status = await this.redis.hgetall(workerKey);
      const lastPing = new Date(status.lastPing);
      const isHealthy = (Date.now() - lastPing.getTime()) < 30000; // 30s timeout
      
      health.workers[workerKey] = {
        healthy: isHealthy,
        lastPing: status.lastPing,
        processed: parseInt(status.processed || '0'),
        errors: parseInt(status.errors || '0')
      };
    }
    
    return health;
  }
}
```

### Dead Letter Queue Management

```typescript
export class DLQManager {
  async processDLQ(queueName: string): Promise<void> {
    const dlqKey = `queue:dlq:${queueName}`;
    const jobs = await this.redis.lrange(dlqKey, 0, -1);
    
    for (const jobData of jobs) {
      const job = JSON.parse(jobData);
      
      // Analyze failure reason
      const canRetry = await this.analyzeFailure(job);
      
      if (canRetry && job.retries < 3) {
        // Retry with exponential backoff
        const delay = Math.pow(2, job.retries) * 1000;
        await this.scheduleRetry(job, delay);
        
        // Remove from DLQ
        await this.redis.lrem(dlqKey, 1, jobData);
      } else {
        // Move to permanent failure storage
        await this.archiveFailedJob(job);
      }
    }
  }

  private async analyzeFailure(job: FailedJob): boolean {
    // Transient errors that can be retried
    const retryableErrors = [
      'TIMEOUT',
      'NETWORK_ERROR',
      'RATE_LIMIT',
      'TEMPORARY_FAILURE'
    ];
    
    return retryableErrors.some(error => job.error.includes(error));
  }

  private async scheduleRetry(job: FailedJob, delay: number): Promise<void> {
    const retryJob = {
      ...job,
      retries: (job.retries || 0) + 1,
      scheduledAt: new Date(Date.now() + delay).toISOString()
    };
    
    // Add to delayed queue
    await this.redis.zadd(
      'queue:delayed',
      Date.now() + delay,
      JSON.stringify(retryJob)
    );
  }
}
```

---

## üìà Performance Monitoring

### Real-Time Metrics Collection

```typescript
export class RedisMetricsCollector {
  async collectMetrics(): Promise<RedisMetrics> {
    const info = await this.redis.info();
    const config = await this.redis.config('get', '*');
    
    const metrics: RedisMetrics = {
      server: this.parseServerInfo(info),
      memory: this.parseMemoryInfo(info),
      stats: this.parseStats(info),
      replication: this.parseReplication(info),
      persistence: this.parsePersistence(info),
      keyspace: await this.analyzeKeyspace(),
      slowlog: await this.getSlowLog()
    };
    
    // Store metrics for historical analysis
    await this.storeMetrics(metrics);
    
    return metrics;
  }

  private parseMemoryInfo(info: string): MemoryMetrics {
    const lines = info.split('\n');
    const memory: MemoryMetrics = {};
    
    lines.forEach(line => {
      if (line.startsWith('used_memory:')) {
        memory.usedMemory = parseInt(line.split(':')[1]);
      } else if (line.startsWith('used_memory_human:')) {
        memory.usedMemoryHuman = line.split(':')[1].trim();
      } else if (line.startsWith('used_memory_peak:')) {
        memory.peakMemory = parseInt(line.split(':')[1]);
      } else if (line.startsWith('mem_fragmentation_ratio:')) {
        memory.fragmentationRatio = parseFloat(line.split(':')[1]);
      }
    });
    
    return memory;
  }

  async analyzeKeyspace(): Promise<KeyspaceAnalysis> {
    const analysis: KeyspaceAnalysis = {
      totalKeys: 0,
      keysByType: {},
      keysByPattern: {},
      largestKeys: []
    };
    
    // Sample keys for analysis
    const sampleSize = 1000;
    const cursor = '0';
    const result = await this.redis.scan(cursor, 'COUNT', sampleSize);
    const keys = result[1];
    
    for (const key of keys) {
      analysis.totalKeys++;
      
      // Get key type
      const type = await this.redis.type(key);
      analysis.keysByType[type] = (analysis.keysByType[type] || 0) + 1;
      
      // Analyze key pattern
      const pattern = this.extractKeyPattern(key);
      analysis.keysByPattern[pattern] = (analysis.keysByPattern[pattern] || 0) + 1;
      
      // Track large keys
      if (type === 'string') {
        const size = await this.redis.strlen(key);
        if (size > 1024 * 1024) { // 1MB
          analysis.largestKeys.push({ key, size, type });
        }
      } else if (type === 'hash') {
        const size = await this.redis.hlen(key);
        if (size > 1000) {
          analysis.largestKeys.push({ key, size, type });
        }
      }
    }
    
    // Sort largest keys
    analysis.largestKeys.sort((a, b) => b.size - a.size);
    analysis.largestKeys = analysis.largestKeys.slice(0, 10);
    
    return analysis;
  }

  async getSlowLog(count: number = 10): Promise<SlowLogEntry[]> {
    const slowlog = await this.redis.slowlog('get', count);
    
    return slowlog.map(entry => ({
      id: entry[0],
      timestamp: new Date(entry[1] * 1000),
      duration: entry[2], // microseconds
      command: entry[3].join(' '),
      clientInfo: entry[4]
    }));
  }

  private extractKeyPattern(key: string): string {
    // Replace dynamic parts with placeholders
    return key
      .replace(/:[a-f0-9-]+/g, ':{id}')
      .replace(/:\d+/g, ':{number}')
      .replace(/:[^:]+@[^:]+/g, ':{email}');
  }
}
```

### Performance Optimization Monitoring

```typescript
export class PerformanceOptimizer {
  async analyzePerformance(): Promise<PerformanceReport> {
    const report: PerformanceReport = {
      timestamp: new Date(),
      issues: [],
      recommendations: [],
      metrics: {}
    };
    
    // Check memory fragmentation
    const memInfo = await this.getMemoryInfo();
    if (memInfo.fragmentationRatio > 1.5) {
      report.issues.push({
        severity: 'high',
        type: 'memory_fragmentation',
        value: memInfo.fragmentationRatio,
        description: 'High memory fragmentation detected'
      });
      report.recommendations.push('Consider restarting Redis during low traffic');
    }
    
    // Check slow queries
    const slowQueries = await this.redis.slowlog('get', 100);
    const slowPatterns = this.analyzeSlowPatterns(slowQueries);
    
    if (slowPatterns.length > 0) {
      report.issues.push({
        severity: 'medium',
        type: 'slow_queries',
        patterns: slowPatterns,
        description: 'Recurring slow query patterns detected'
      });
      report.recommendations.push('Optimize identified query patterns');
    }
    
    // Check key expiration
    const expirationStats = await this.analyzeExpiration();
    if (expirationStats.expiredRatio > 0.3) {
      report.issues.push({
        severity: 'low',
        type: 'high_expiration',
        value: expirationStats.expiredRatio,
        description: 'High key expiration rate'
      });
      report.recommendations.push('Review TTL settings');
    }
    
    // Check connection usage
    const connInfo = await this.getConnectionInfo();
    if (connInfo.connectedClients > connInfo.maxClients * 0.8) {
      report.issues.push({
        severity: 'high',
        type: 'connection_limit',
        value: connInfo.connectedClients,
        max: connInfo.maxClients,
        description: 'Approaching connection limit'
      });
      report.recommendations.push('Increase max client connections or implement connection pooling');
    }
    
    return report;
  }

  private analyzeSlowPatterns(slowlog: any[]): SlowPattern[] {
    const patterns = new Map<string, number>();
    
    slowlog.forEach(entry => {
      const command = entry[3][0]; // Command name
      const pattern = this.generalizeCommand(entry[3]);
      
      patterns.set(pattern, (patterns.get(pattern) || 0) + 1);
    });
    
    return Array.from(patterns.entries())
      .filter(([_, count]) => count > 5) // Recurring patterns
      .map(([pattern, count]) => ({
        pattern,
        count,
        avgDuration: this.calculateAvgDuration(slowlog, pattern)
      }))
      .sort((a, b) => b.count - a.count);
  }
}
```

---

## üñ•Ô∏è Admin Interface Requirements

### Core Dashboard Components

```typescript
interface RedisAdminDashboard {
  // Overview Section
  overview: {
    serverInfo: {
      version: string;
      uptime: number;
      connectedClients: number;
      usedMemory: string;
      peakMemory: string;
      keyspaceHits: number;
      keyspaceMisses: number;
    };
    
    realtimeMetrics: {
      opsPerSecond: number;
      networkBandwidth: { in: number; out: number };
      cpuUsage: number;
      hitRate: number;
    };
  };
  
  // Key Management
  keyManagement: {
    search: {
      pattern: string;
      type?: string;
      minSize?: number;
      maxSize?: number;
    };
    
    browser: {
      tree: KeyTreeNode[];
      preview: {
        key: string;
        type: string;
        ttl: number;
        size: number;
        value: any;
      };
    };
    
    operations: {
      edit: (key: string, value: any) => Promise<void>;
      delete: (keys: string[]) => Promise<void>;
      expire: (key: string, ttl: number) => Promise<void>;
      rename: (oldKey: string, newKey: string) => Promise<void>;
    };
  };
  
  // Queue Management
  queueManagement: {
    overview: {
      queues: QueueInfo[];
      workers: WorkerInfo[];
      throughput: { [queue: string]: number };
    };
    
    operations: {
      pause: (queue: string) => Promise<void>;
      resume: (queue: string) => Promise<void>;
      clear: (queue: string) => Promise<void>;
      reprocessDLQ: (queue: string) => Promise<void>;
    };
    
    jobInspector: {
      view: (jobId: string) => Promise<JobDetails>;
      retry: (jobId: string) => Promise<void>;
      delete: (jobId: string) => Promise<void>;
    };
  };
  
  // Performance Analytics
  analytics: {
    commandStats: {
      topCommands: { command: string; calls: number; usec: number }[];
      slowQueries: SlowLogEntry[];
    };
    
    keyspaceAnalytics: {
      distribution: { [pattern: string]: number };
      growth: { timestamp: Date; count: number }[];
      largestKeys: { key: string; size: number; type: string }[];
    };
    
    memoryAnalytics: {
      usage: TimeSeriesData;
      fragmentation: TimeSeriesData;
      evictions: TimeSeriesData;
    };
  };
  
  // Cache Management
  cacheManagement: {
    hitRate: {
      overall: number;
      byLayer: { [layer: string]: number };
      byPattern: { [pattern: string]: number };
    };
    
    operations: {
      warm: (pattern: string) => Promise<void>;
      invalidate: (pattern: string) => Promise<void>;
      analyze: (pattern: string) => Promise<CacheAnalysis>;
    };
  };
  
  // Configuration
  configuration: {
    view: () => Promise<RedisConfig>;
    update: (config: Partial<RedisConfig>) => Promise<void>;
    backup: () => Promise<ConfigBackup>;
    restore: (backup: ConfigBackup) => Promise<void>;
  };
  
  // Monitoring & Alerts
  monitoring: {
    alerts: {
      rules: AlertRule[];
      history: Alert[];
      configure: (rule: AlertRule) => Promise<void>;
    };
    
    health: {
      status: 'healthy' | 'degraded' | 'unhealthy';
      checks: HealthCheck[];
      recommendations: string[];
    };
  };
}
```

### UI Component Specifications

```typescript
// Real-time Metrics Display
export const MetricsDisplay: React.FC = () => {
  return (
    <div className="metrics-grid">
      <MetricCard
        title="Operations/sec"
        value={metrics.opsPerSecond}
        trend={metrics.opsTrend}
        format="number"
      />
      <MetricCard
        title="Memory Usage"
        value={metrics.memoryUsage}
        max={metrics.maxMemory}
        format="bytes"
        showProgress
      />
      <MetricCard
        title="Hit Rate"
        value={metrics.hitRate}
        format="percentage"
        status={metrics.hitRate > 0.9 ? 'good' : 'warning'}
      />
      <MetricCard
        title="Connected Clients"
        value={metrics.connectedClients}
        max={metrics.maxClients}
        format="number"
      />
    </div>
  );
};

// Key Browser Component
export const KeyBrowser: React.FC = () => {
  const [selectedKey, setSelectedKey] = useState<string | null>(null);
  const [keyData, setKeyData] = useState<any>(null);
  
  return (
    <div className="key-browser">
      <div className="key-tree">
        <KeyTree
          onSelect={setSelectedKey}
          onDelete={handleDelete}
          onRefresh={refreshKeys}
        />
      </div>
      
      <div className="key-preview">
        {selectedKey && (
          <KeyPreview
            keyName={selectedKey}
            data={keyData}
            onEdit={handleEdit}
            onDelete={() => handleDelete([selectedKey])}
            onExpire={handleExpire}
          />
        )}
      </div>
    </div>
  );
};

// Queue Monitor Component
export const QueueMonitor: React.FC = () => {
  return (
    <div className="queue-monitor">
      <QueueOverview queues={queues} />
      
      <div className="queue-details">
        {selectedQueue && (
          <>
            <QueueStats queue={selectedQueue} />
            <JobList
              queue={selectedQueue}
              onInspect={inspectJob}
              onRetry={retryJob}
            />
            <WorkerStatus workers={workers} />
          </>
        )}
      </div>
      
      <DLQManager
        queues={dlQueues}
        onReprocess={reprocessDLQ}
        onClear={clearDLQ}
      />
    </div>
  );
};
```

### Admin API Endpoints

```typescript
// Admin API Routes
export const AdminAPI = {
  // Dashboard
  'GET /api/admin/redis/overview': {
    response: RedisOverview
  },
  
  'GET /api/admin/redis/metrics': {
    query: { period: string },
    response: RedisMetrics[]
  },
  
  // Key Management
  'GET /api/admin/redis/keys': {
    query: { pattern: string, cursor?: string, count?: number },
    response: { keys: string[], cursor: string }
  },
  
  'GET /api/admin/redis/key/:key': {
    params: { key: string },
    response: KeyDetails
  },
  
  'PUT /api/admin/redis/key/:key': {
    params: { key: string },
    body: { value: any, ttl?: number },
    response: { success: boolean }
  },
  
  'DELETE /api/admin/redis/keys': {
    body: { keys: string[] },
    response: { deleted: number }
  },
  
  // Queue Management
  'GET /api/admin/redis/queues': {
    response: QueueInfo[]
  },
  
  'POST /api/admin/redis/queue/:name/pause': {
    params: { name: string },
    response: { success: boolean }
  },
  
  'POST /api/admin/redis/queue/:name/clear': {
    params: { name: string },
    response: { cleared: number }
  },
  
  // Performance
  'GET /api/admin/redis/slowlog': {
    query: { count?: number },
    response: SlowLogEntry[]
  },
  
  'GET /api/admin/redis/memory': {
    response: MemoryAnalysis
  },
  
  // Configuration
  'GET /api/admin/redis/config': {
    response: RedisConfig
  },
  
  'POST /api/admin/redis/config': {
    body: Partial<RedisConfig>,
    response: { success: boolean }
  }
};
```

---

## üîß Troubleshooting Guide

### Common Issues and Solutions

```typescript
export const TroubleshootingGuide = {
  // High Memory Usage
  highMemoryUsage: {
    symptoms: [
      'Memory usage > 80% of max memory',
      'Increased evictions',
      'Performance degradation'
    ],
    
    diagnosis: async () => {
      // Check memory stats
      const info = await redis.info('memory');
      
      // Analyze large keys
      const largeKeys = await findLargeKeys();
      
      // Check for memory leaks
      const leaks = await detectMemoryLeaks();
      
      return { info, largeKeys, leaks };
    },
    
    solutions: [
      'Increase maxmemory setting',
      'Review and optimize key expiration',
      'Implement more aggressive eviction policies',
      'Archive old data to persistent storage',
      'Optimize data structures (use hashes instead of strings)'
    ]
  },
  
  // Slow Queries
  slowQueries: {
    symptoms: [
      'High latency on operations',
      'Timeout errors',
      'Queue backlogs'
    ],
    
    diagnosis: async () => {
      // Get slow log
      const slowlog = await redis.slowlog('get', 100);
      
      // Analyze patterns
      const patterns = analyzeSlowPatterns(slowlog);
      
      // Check blocking operations
      const blocking = await findBlockingOperations();
      
      return { slowlog, patterns, blocking };
    },
    
    solutions: [
      'Avoid KEYS command in production',
      'Use SCAN instead of SMEMBERS for large sets',
      'Implement pagination for large data retrievals',
      'Consider using Redis modules for complex operations',
      'Add indexes for frequently accessed patterns'
    ]
  },
  
  // Connection Issues
  connectionIssues: {
    symptoms: [
      'Connection refused errors',
      'Max clients reached',
      'Intermittent timeouts'
    ],
    
    diagnosis: async () => {
      // Check connection stats
      const clients = await redis.client('list');
      const connInfo = await redis.info('clients');
      
      // Analyze connection patterns
      const patterns = analyzeConnectionPatterns(clients);
      
      return { clients, connInfo, patterns };
    },
    
    solutions: [
      'Increase maxclients setting',
      'Implement connection pooling',
      'Add connection timeout handling',
      'Use persistent connections',
      'Monitor and close idle connections'
    ]
  },
  
  // Replication Lag
  replicationLag: {
    symptoms: [
      'Slave lag > 1 second',
      'Inconsistent reads',
      'Replication buffer overflow'
    ],
    
    diagnosis: async () => {
      // Check replication info
      const replInfo = await redis.info('replication');
      
      // Monitor lag over time
      const lagHistory = await getReplicationLagHistory();
      
      return { replInfo, lagHistory };
    },
    
    solutions: [
      'Increase replication buffer size',
      'Optimize network between master and slaves',
      'Reduce write load on master',
      'Consider using Redis Cluster for sharding',
      'Enable diskless replication for faster syncs'
    ]
  }
};

// Diagnostic Functions
async function findLargeKeys(threshold = 1024 * 1024): Promise<LargeKey[]> {
  const largeKeys: LargeKey[] = [];
  let cursor = '0';
  
  do {
    const [nextCursor, keys] = await redis.scan(cursor, 'COUNT', 100);
    
    for (const key of keys) {
      const type = await redis.type(key);
      let size = 0;
      
      switch (type) {
        case 'string':
          size = await redis.strlen(key);
          break;
        case 'list':
          size = await redis.llen(key);
          break;
        case 'set':
          size = await redis.scard(key);
          break;
        case 'zset':
          size = await redis.zcard(key);
          break;
        case 'hash':
          size = await redis.hlen(key);
          break;
      }
      
      if (size > threshold) {
        largeKeys.push({ key, type, size });
      }
    }
    
    cursor = nextCursor;
  } while (cursor !== '0');
  
  return largeKeys.sort((a, b) => b.size - a.size);
}

async function detectMemoryLeaks(): Promise<MemoryLeak[]> {
  const leaks: MemoryLeak[] = [];
  
  // Check for keys without TTL
  const noTTLKeys = await findKeysWithoutTTL();
  if (noTTLKeys.length > 1000) {
    leaks.push({
      type: 'no-ttl',
      severity: 'high',
      count: noTTLKeys.length,
      recommendation: 'Set TTL for temporary data'
    });
  }
  
  // Check for growing lists/sets
  const growingCollections = await findGrowingCollections();
  if (growingCollections.length > 0) {
    leaks.push({
      type: 'unbounded-growth',
      severity: 'medium',
      keys: growingCollections,
      recommendation: 'Implement size limits or archival'
    });
  }
  
  return leaks;
}
```

---

## üöÄ Best Practices & Optimization

### Key Design Best Practices

```typescript
export const RedisbestPractices = {
  // Key Naming
  keyNaming: {
    useNamespaces: true,
    separator: ':',
    examples: {
      good: 'user:123:profile',
      bad: 'user_123_profile'
    },
    
    guidelines: [
      'Use consistent separators',
      'Include version in key for schema changes',
      'Keep keys short but descriptive',
      'Avoid special characters except : and -'
    ]
  },
  
  // Data Structure Selection
  dataStructures: {
    useHashesForObjects: {
      instead: 'user:123:name, user:123:email',
      use: 'user:123 as hash with name, email fields',
      benefit: '~10x memory savings'
    },
    
    useSortedSetsForRanking: {
      example: 'leaderboard:daily',
      operations: ['ZADD', 'ZRANGE', 'ZRANK'],
      benefit: 'O(log n) operations'
    },
    
    useListsForQueues: {
      pattern: 'LPUSH/RPOP for FIFO',
      benefit: 'Atomic operations, reliable processing'
    }
  },
  
  // TTL Strategy
  ttlStrategy: {
    always: ['cache entries', 'session data', 'temporary locks'],
    never: ['user profiles', 'permanent settings'],
    conditional: ['analysis results - TTL based on size', 'logs - TTL based on level']
  },
  
  // Pipeline Usage
  pipelining: {
    when: 'Multiple operations on different keys',
    example: `
      const pipeline = redis.pipeline();
      keys.forEach(key => pipeline.get(key));
      const results = await pipeline.exec();
    `,
    benefit: 'Reduces round trips, ~5-10x performance improvement'
  },
  
  // Connection Pooling
  connectionPooling: {
    recommended: {
      min: 5,
      max: 30,
      acquireTimeoutMillis: 3000,
      idleTimeoutMillis: 30000
    }
  }
};
```

### Memory Optimization Techniques

```typescript
export class MemoryOptimizer {
  // Use Redis memory efficiently
  optimizationTechniques = {
    // 1. Use appropriate data types
    stringCompression: {
      before: 'SET user:123:data "large json string"',
      after: 'SET user:123:data "compressed data" // Use compression',
      savings: '~70% for JSON data'
    },
    
    // 2. Use hashes for small objects
    hashPacking: {
      before: `
        SET user:123:name "John"
        SET user:123:email "john@example.com"
        SET user:123:age "25"
      `,
      after: `
        HSET user:123 name "John" email "john@example.com" age "25"
      `,
      savings: '~10x memory reduction'
    },
    
    // 3. Configure hash-max-ziplist
    configuration: {
      'hash-max-ziplist-entries': 512,
      'hash-max-ziplist-value': 64,
      benefit: 'Automatic compression for small hashes'
    },
    
    // 4. Use integer encoding
    integerOptimization: {
      before: 'SET counter "12345"',
      after: 'SET counter 12345',
      benefit: 'Integer encoding saves memory'
    },
    
    // 5. Implement data archival
    archival: async () => {
      // Move old data to persistent storage
      const oldKeys = await this.findOldKeys(30); // 30 days
      
      for (const key of oldKeys) {
        const data = await redis.get(key);
        await this.archiveToS3(key, data);
        await redis.del(key);
      }
    }
  };
}
```

### Performance Optimization Patterns

```typescript
export const PerformancePatterns = {
  // Lua Scripts for Atomic Operations
  luaScripts: {
    conditionalSet: `
      local key = KEYS[1]
      local value = ARGV[1]
      local condition = ARGV[2]
      
      local current = redis.call('GET', key)
      if current == condition then
        return redis.call('SET', key, value)
      else
        return nil
      end
    `,
    
    benefit: 'Atomic operations, reduced network overhead'
  },
  
  // Batch Operations
  batchOperations: {
    example: async (keys: string[]) => {
      // Instead of individual operations
      // for (const key of keys) await redis.get(key);
      
      // Use MGET
      const values = await redis.mget(...keys);
      return keys.map((key, i) => ({ key, value: values[i] }));
    }
  },
  
  // Smart Caching Patterns
  cachingPatterns: {
    cacheAside: async (key: string) => {
      let value = await redis.get(key);
      if (!value) {
        value = await fetchFromDatabase(key);
        await redis.setex(key, 3600, value);
      }
      return value;
    },
    
    writeThrough: async (key: string, value: any) => {
      await redis.set(key, value);
      await saveToDatabase(key, value);
    },
    
    writeBehind: async (key: string, value: any) => {
      await redis.set(key, value);
      await redis.lpush('write-queue', JSON.stringify({ key, value }));
      // Process write-queue asynchronously
    }
  },
  
  // Connection Optimization
  connectionOptimization: {
    keepAlive: true,
    noDelay: true,
    connectionTimeout: 5000,
    commandTimeout: 5000,
    reconnectOnError: (err: Error) => {
      const targetErrors = ['READONLY', 'ECONNRESET'];
      return targetErrors.some(e => err.message.includes(e));
    }
  }
};
```

---

## üéØ Admin Tool Implementation Requirements

### Essential Features Checklist

```typescript
interface AdminToolRequirements {
  // Core Features (Must Have)
  core: {
    authentication: {
      type: 'role-based',
      roles: ['admin', 'developer', 'readonly'],
      mfa: true
    },
    
    dashboard: {
      realTimeMetrics: true,
      keyBrowser: true,
      queueMonitor: true,
      slowLogViewer: true
    },
    
    operations: {
      keyManagement: ['view', 'edit', 'delete', 'expire'],
      queueManagement: ['pause', 'resume', 'clear', 'reprocess'],
      bulkOperations: true,
      search: { pattern: true, fuzzy: true }
    }
  },
  
  // Advanced Features (Should Have)
  advanced: {
    analytics: {
      memoryAnalysis: true,
      performanceTrends: true,
      keyspaceAnalytics: true,
      costEstimation: true
    },
    
    automation: {
      scheduledTasks: true,
      alerts: { email: true, slack: true, webhook: true },
      autoScaling: true,
      backupScheduling: true
    },
    
    debugging: {
      queryProfiler: true,
      memoryProfiler: true,
      connectionDebugger: true,
      replicationMonitor: true
    }
  },
  
  // Nice to Have
  niceToHave: {
    visualization: {
      keyHeatmap: true,
      memoryTreemap: true,
      queueFlowDiagram: true
    },
    
    collaboration: {
      sharedDashboards: true,
      annotations: true,
      changeHistory: true
    },
    
    export: {
      formats: ['csv', 'json', 'pdf'],
      scheduling: true,
      templates: true
    }
  }
}
```

### Technology Stack Recommendations

```typescript
const recommendedStack = {
  frontend: {
    framework: 'React 18+ with TypeScript',
    ui: 'Ant Design or Material-UI',
    state: 'Redux Toolkit + RTK Query',
    charts: 'Recharts or D3.js',
    realtime: 'Socket.io or Server-Sent Events'
  },
  
  backend: {
    framework: 'Node.js + Express or Fastify',
    redis: 'ioredis (recommended) or node-redis',
    auth: 'JWT + Redis sessions',
    api: 'REST + GraphQL for complex queries',
    monitoring: 'Prometheus + Grafana integration'
  },
  
  deployment: {
    containerization: 'Docker',
    orchestration: 'Kubernetes or Docker Compose',
    reverseProxy: 'Nginx',
    ssl: 'Let\'s Encrypt',
    monitoring: 'ELK Stack or Datadog'
  }
};
```

---

## üìö Conclusion

This comprehensive Redis management documentation provides everything needed to understand, manage, and build admin tools for the Resume Processing Platform's Redis infrastructure. The system leverages Redis for high-performance caching, reliable queue processing, and real-time operations while maintaining data integrity and operational excellence.

Key architectural decisions:
- Multi-layer caching for optimal performance
- Priority-based queue system for fair processing
- Comprehensive monitoring and alerting
- Scalable key naming conventions
- Memory-optimized data structures

The admin interface requirements prioritize developer experience while providing powerful tools for monitoring, debugging, and optimization. Implementation should focus on real-time metrics, intuitive key management, and robust queue monitoring capabilities.

---

**Version**: 1.0.0  
**Last Updated**: January 2025  
**Component**: Redis Infrastructure  
**Platform**: AI Resume Processing System
